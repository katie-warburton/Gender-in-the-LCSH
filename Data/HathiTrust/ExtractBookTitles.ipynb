{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what each data entry means: https://www.hathitrust.org/member-libraries/resources-for-librarians/data-resources/hathifiles/hathifiles-description/ \n",
    "Key indices \n",
    "- 11 = Title\n",
    "- 16 = Publication date (if date == 9999 it means it can't be determined from the source)\n",
    "- 18 = Language (eng = English)\n",
    "- 25 = author (although not all records have an author)\n",
    "\n",
    "Right now I'm limiting to books (data[19]== 'bk') as I'm not sure how to handle serials and such (same name but multiple publications) and I want to start with a smaller portion of the dataset\n",
    "Later I might expand to other kinds of publications (i.e. Serials, electronic resources, maps, music, visual material, etc.)\n",
    "\n",
    "Originally I was grabbing all file formats but books make up the majority of the data (~6 million with all formats vs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 records processed\n",
      "4000000 records processed\n",
      "5000000 records processed\n",
      "8000000 records processed\n",
      "9000000 records processed\n"
     ]
    }
   ],
   "source": [
    "books = []\n",
    "bookHash = {}\n",
    "i = 0\n",
    "with gzip.open('hathi_full_20240801.txt.gz', 'r') as f:\n",
    "    for line in f:\n",
    "        data = line.decode('utf-8').strip().split('\\t')\n",
    "        if data[18] != 'eng' or data[19].lower() != 'bk':\n",
    "            i += 1\n",
    "            continue\n",
    "        title, pubDate =  data[11], data[16]\n",
    "        if len(data) == 26:\n",
    "            auth = data[-1]\n",
    "        else:\n",
    "            auth = ''\n",
    "        bookData = [title, pubDate, auth]\n",
    "        if auth not in bookHash:\n",
    "            bookHash[auth] = {title: {pubDate: 1}}\n",
    "            books.append(bookData) # only add unique books to list (diff author and title and publication date)\n",
    "        else:\n",
    "            if title not in bookHash[auth]:\n",
    "                bookHash[auth][title] = {pubDate: 1}\n",
    "                books.append(bookData) # same author, diff title\n",
    "            else:\n",
    "                if pubDate not in bookHash[auth][title]: # same author, same title, diff publication date\n",
    "                    bookHash[auth][title][pubDate] = 1\n",
    "                    books.append(bookData)\n",
    "                else: # assume same book\n",
    "                    bookHash[auth][title][pubDate] += 1\n",
    "        i += 1\n",
    "        if i % 1000000 == 0:\n",
    "            print(f'{i} records processed')\n",
    "print(len(books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksWithCount = {}\n",
    "i = 0\n",
    "for bookInfo in books:\n",
    "    title, pubDate, auth = bookInfo\n",
    "    count = bookHash[auth][title][pubDate]\n",
    "    booksWithCount[i] = {'title':title, 'date':pubDate, 'author':auth,'copies':count}\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('englishHathi.pkl', 'wb') as f:\n",
    "    pickle.dump(booksWithCount, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Proj4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
